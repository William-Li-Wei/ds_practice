{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage\n",
    "\n",
    "In reinforcement learning, the agent generates its own training data by interacting with the world. The agent must learn the consequences of his own actions through trial and error, rather than being told the correct action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. K-armed bandits\n",
    "\n",
    "Let's start with a simple case of **`decision-making under uncertainty`**.\n",
    "\n",
    "In the **`K-armed bandit problem`**:\n",
    "- we have an **`agent`**\n",
    "- who chooses between **`k actions`**\n",
    "- and receives a **`reward`** based on the action it chooses.\n",
    "\n",
    "<img src=\"resources/k_armed_bandit.png\" alt=\"Drawing\" style=\"width: 400px; margin-left: 4em\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 The Action Values\n",
    "\n",
    "### 1.1.1 The real Action Values q*(a)\n",
    "We define the value of selecting an action as the expected reward we receive when taking that action.\n",
    "\n",
    "**`q*(a) = E[ R | A = a ] = Sum[ p(r|a)*r ]`**, for each possible action 1 through k.\n",
    "\n",
    "<img src=\"resources/q*_and_argmax(q*).png\" style=\"width: 400px; margin-left: 4em\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Estimated Action values Q(a)\n",
    "\n",
    "However, q*(a) is unknow in most problems, we need to estimate it.\n",
    "\n",
    "**`Q(a, t) = Sum(R) / (t - 1)`**\n",
    "\n",
    "<img src=\"resources/Q(a)_sample_average.png\" style=\"width: 400px; margin-left: 4em\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 Estimating Action Values Incrementally\n",
    "\n",
    "**`Q(n + 1) = Sum(R) / n = Q(n) + [R(n)- Q(n)] / n`**\n",
    "\n",
    "<img src=\"resources/Q(a)_incremental_update.png\" style=\"width: 400px; margin-left: 4em\"/>\n",
    "\n",
    "<img src=\"resources/Q(a)_incremental_update_rule.png\" style=\"width: 400px; margin-left: 4em\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.4 Constant stepsize could solve the non-stationary problem\n",
    "\n",
    "the influence of initialization of Q goes to zero with more and more data. **`The most recent rewards contribute most`** to our current estimate\n",
    "\n",
    "<img src=\"resources/Q(a)_decaying_past_rewards.png\" style=\"width: 400px; margin-left: 4em\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Action Selection\n",
    "\n",
    "### 1.2.1 The Greedy Action\n",
    "\n",
    "The greedy action is the action that currently has **`the largest estimated value`**.\n",
    "\n",
    "### 1.2.2 Exploitation\n",
    "\n",
    "**`Selecting the greedy action`** means the agent is exploiting its current knowledge. It is trying to **`get the most reward`** it can **`right now`**. We can compute the greedy action **`by taking the argmax`** of our estimated values.\n",
    "\n",
    "### 1.2.3 Exploration\n",
    "\n",
    "Alternatively, the agent may choose to explore by **`choosing a non-greedy action`**. The agent would **`sacrifice immediate reward`**, hoping to gain more information about the other actions for **`potential long term rewards`**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.4 Optimistic Initial Value\n",
    "\n",
    "- encourage early exploration\n",
    "- cannot adapt to non-stationay problem. No exploration at larger steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.5 Epsilon-Greedy\n",
    "\n",
    "- works for non-stationary problem.\n",
    "- however, cannot make full use of exploitation because of epsilon\n",
    "\n",
    "<img src=\"resources/Action_selection_Epsilon_Greedy.png\" style=\"width: 400px; margin-left: 4em\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.6 Upper Confidence Bound\n",
    "\n",
    "- Initially, UCB expore more to systematically reduce uncertainty \n",
    "\n",
    "- UCB exploration reduce over time, thus doesn't fit to non-stationary problem too\n",
    "\n",
    "<img src=\"resources/Action_selection_Upper_Confidence_Bound.png\" style=\"width: 400px; margin-left: 4em\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Markov Decision Process (MDP)\n",
    "\n",
    "The K-armed bandit problem doesn't include many aspects of real-world problems. The agent is presented with the same situation and each time and the same action is always optimal. \n",
    "\n",
    "In many problems, different situations call for different responses. The actions we choose now affect the amount of reward we can get into the future.\n",
    "\n",
    "<img src=\"resources/MDP.png\" style=\"width: 600px; margin-left: 4em\"/>\n",
    "\n",
    "This diagram summarizes the agent environment interaction in the MDP framework. The agent environment interaction generates a trajectory of experience consisting of states, actions, and rewards. Actions influence immediate rewards as well as future states and through those, future reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Dynamics of MDP\n",
    "\n",
    "Given a state S and action a, p tells us the joint probability of next state S prime and reward R.\n",
    "\n",
    "<img src=\"resources/MDP_Dynamics.png\" style=\"width: 600px; margin-left: 4em\"/>\n",
    "\n",
    "The following example shows the Dynamics of the Soda Can Robot problem.\n",
    "\n",
    "States: Battery High, Battery Low\n",
    "\n",
    "Actions: Wait, Search, Recharge\n",
    "\n",
    "\n",
    "<img src=\"resources/MDP_Example.png\" style=\"width: 600px; margin-left: 4em\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Goal: Maximize the expected return\n",
    "\n",
    "Return at time step t, is the **`sum of rewards`** obtained **`after time step t`** \n",
    "\n",
    "**`G(t) = R(t+1) + R(t+2) + R(t+3) + …`**\n",
    "\n",
    "**`E[G(t)] = E[ R(t+1) + R(t+2) + R(t+3) + … ]`**\n",
    "\n",
    "- Episodic task: the agent-environment interaction breaks up into episodes. \n",
    "- Continuing task: the agent-environment interaction goes on indefinitely. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Discounting factor`** gama, makes sure the **`sum of all future goals are finite`**.\n",
    "\n",
    "- **`smaller gama`**, makes the agent **`short sighted`** since it concerns more on immediate rewards.\n",
    "- **`greater gama`**, considers the **`long term rewards`** more.\n",
    "\n",
    "<img src=\"resources/Recursive_nature_of_expected_goal.png\" style=\"width: 400px; margin-left: 4em\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Policy: A distribution over actions for each possible state.\n",
    "\n",
    "- Deterministic policy: maps each state to a single action \n",
    "- Stochastic policy: assigns possibility to each action at each state \n",
    "- A Valid policy depends on only the current state \n",
    "\n",
    "<img src=\"resources/Policy_notation.png\" style=\"width: 600px; margin-left: 4em\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Value functions\n",
    "\n",
    "Value functions are crucial in reinforce learning, they allow an agent to **`query the quality of its current situation`** instead of waiting to observe the long-term outcome. \n",
    "\n",
    "The benefit is twofold:\n",
    "\n",
    "- First, the return is not immediately available\n",
    "- Second, the return may be random due to stochasticity in both the policy and environment dynamics. \n",
    "\n",
    "The value function **`summarizes all the possible futures`** by averaging over returns. \n",
    "\n",
    "Ultimately, we **`care most about learning a good policy`**. Value function enable us to **`judge the quality of different policies`**.\n",
    "\n",
    "### 2.4.1 State Value functions\n",
    "\n",
    "is the expected return from a given state, with respect to the policy Pi.\n",
    "\n",
    "<img src=\"resources/State_value_function.png\" style=\"width: 400px; margin-left: 4em\"/>\n",
    "\n",
    "### 2.4.2 Action Value functions\n",
    "\n",
    "the action value of a state s is the expected return if the agent selects action a and then follows policy Pi.\n",
    "\n",
    "<img src=\"resources/Action_value_function.png\" style=\"width: 400px; margin-left: 4em\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Bellman equations of Value functions\n",
    "\n",
    "They provide **`relationships`** between the **`values of a state or state action pair`** and the **`possible next states or next state action pairs`**\n",
    "\n",
    "### 2.5.1 State value Bellman euqations\n",
    "\n",
    "<img src=\"resources/State_value_Bellman_euqation.png\" style=\"width: 600px; margin-left: 4em\"/>\n",
    "\n",
    "### 2.5.2 Action value Bellman equations\n",
    "\n",
    "<img src=\"resources/Action_value_Bellman_equation.png\" style=\"width: 600px; margin-left: 4em\"/>\n",
    "\n",
    "### 2.5.3 Why Bellman equations?\n",
    "\n",
    "Can use the Bellman equations to **`compute value functions`**.\n",
    "\n",
    "- You can use the Bellman equations to **`solve a value function`** by writing **`a system of linear equations`**.\n",
    "- We can **`only solve small MDPs`** directly with Bellman equations.\n",
    "\n",
    "<img src=\"resources/Bellman_equation_example.png\" style=\"width: 600px; margin-left: 4em\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Optimal Policy and Optimal value functions\n",
    "\n",
    "- An **`optimal policy`** is defined as the policy with the **`highest possible value`** function in **`all states`**. \n",
    "- **`At lease one optimal policy`** always exists, but there **`may be more`** than one. \n",
    "- The exponential number of possible policies makes searching for the optimal policy by brute-force intractable. \n",
    "\n",
    "A **`deterministic optimal policy`** will assign **`Probability 1`**, for an action that achieves the **`highest value`** and Probability **`0, for all other actions`**.\n",
    "\n",
    "Finding an Optimal Policy is the goal of our Reinforcement Learning problems.\n",
    "\n",
    "### 2.6.1 Bellman optimality equation for v*\n",
    "\n",
    "<img src=\"resources/Optimal_State_value_function_Bellman.png\" style=\"width: 400px; margin-left: 4em\"/>\n",
    "\n",
    "### 2.6.2 Bellman optimality equation for q*\n",
    "\n",
    "<img src=\"resources/Optimal_Action_value_function_Bellman.png\" style=\"width: 400px; margin-left: 4em\"/>\n",
    "\n",
    "The Bellman optimality equations **`relate the value of a state`** or **`state-action pair`**, **`to it's possible successors`** under any optimal policy.\n",
    "\n",
    "### 2.6.3 Determin optimal policy\n",
    "\n",
    "In general, **`having v* or q*`** makes it relatively easy to **`work out the optimal policy`** as long as we also have **`access to the dynamics function p`**.\n",
    "\n",
    "<img src=\"resources/Determin_optimal_policy_from_optimal_value_functions.png\" style=\"width: 600px; margin-left: 4em\"/>\n",
    "\n",
    "However, v* and q* are unknown, we need to start from the initial policy and then iterate between Policy Evaluation and Policy Improvement to get the optimoal policy. See more detils in Dynamic Programming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Dynamic Programming\n",
    "\n",
    "- **`Policy evaluation`** is the task of **`determining the state-vale`** function V(Pi), for a particular policy Pi. \n",
    "- **`Control`** is the task of **`improving an existing policy`**. \n",
    "\n",
    "Dynamic programming techniques can be used to solve both these tasks, if we **`have access to the dynamics function P`**. \n",
    "\n",
    "## 3.1 Iterative Policy Evaluation\n",
    "\n",
    "We take the Bellman equation and directly use it as an update rule. This will produce a sequence of better and better approximations to the value function.\n",
    "\n",
    "<img src=\"resources/Policy_Evaluation_1.png\" style=\"width: 400px; margin-left: 4em\"/>\n",
    "\n",
    "It can be proven that for any choice of v(0), v(k) will converge to v(pi) in the limit as k approaches infinity.\n",
    "\n",
    "<img src=\"resources/Policy_Evaluation_2.png\" style=\"width: 400px; margin-left: 4em\"/>\n",
    "\n",
    "The complete iteration of Policy Evaluation is as followed:\n",
    "\n",
    "<img src=\"resources/Policy_Evaluation_3.png\" style=\"width: 600px; margin-left: 4em\"/>\n",
    "\n",
    "### 3.1.2 Policy Evaluation Example\n",
    "\n",
    "In the grid world example, the top-left and the bottom-right blocks are the termination states. The actions are going up, right, down or left from each state, hitting the boundry makes the state unchanged. The policy Pi is the uniform random policy. The initial value of each state under Pi is 0.\n",
    "\n",
    "Calculating the first state gives -1.\n",
    "\n",
    "<img src=\"resources/Policy_Evaluation_Example_1.png\" style=\"width: 600px; margin-left: 4em\"/>\n",
    "\n",
    "After calculating the values of all states (all -1 in this example), we finished one sweep. The max difference delta here is 1.\n",
    "\n",
    "<img src=\"resources/Policy_Evaluation_Example_2.png\" style=\"width: 600px; margin-left: 4em\"/>\n",
    "\n",
    "Repeating the above iteration for another sweep we got:\n",
    "\n",
    "<img src=\"resources/Policy_Evaluation_Example_3.png\" style=\"width: 600px; margin-left: 4em\"/>\n",
    "\n",
    "If we set the minimal difference theta to 0.001 and continue the iteration, eventually we will get the estimated policy evaluation:\n",
    "\n",
    "<img src=\"resources/Policy_Evaluation_Example_4.png\" style=\"width: 600px; margin-left: 4em\"/>\n",
    "\n",
    "\n",
    "\n",
    "## 3.2 Policy Improvement\n",
    "\n",
    "Recall that given v*, we can find the optimal policy by choosing the Greedy action, which maximizes the Bellman's optimality equation in each state.\n",
    "\n",
    "Selecting the **`greedy action`** with respect to the value function v(pi), must **`result in a different policy`** other than pi. **`If this greedification doesn't change pi`**, **`then pi was already greedy`** with respect to its own value function, in other words, pi obeys the Bellman's optimal equition and **`therefore is already optimal`**.\n",
    "\n",
    "<img src=\"resources/Policy_Improvement_1.png\" style=\"width: 600px; margin-left: 4em\"/>\n",
    "\n",
    "### 3.2.1 Policy Improvement Theorem\n",
    "\n",
    "The new policy obtained in the above way must be a strict improvement on Pi, unless Pi was already optimal. This is a consequence of the policy improvement theorem.\n",
    "\n",
    "<img src=\"resources/Policy_Improvement_Theorem.png\" style=\"width: 400px; margin-left: 4em\"/>\n",
    "\n",
    "### 3.2.2 Policy Improvement Example\n",
    "\n",
    "Pi is the uniform random policy, and the numbers are the evaluation of each state. The arrows show the argmax actions and form a new polic Pi*.\n",
    "\n",
    "<img src=\"resources/Policy_Improvement_Example.png\" style=\"width: 600px; margin-left: 4em\"/>\n",
    "\n",
    "## 3.3 Policy Iteration\n",
    "\n",
    "- start from the initial policy Pi1\n",
    "- evaluate the value function V(Pi1)\n",
    "- then take argmax actions to get a better policy Pi2\n",
    "- then evaluate Pi2 and take argmax actions to get Pi3\n",
    "- repeat the process until we get the optimal policy Pi*\n",
    "\n",
    "<img src=\"resources/Policy_Iteration_1.png\" style=\"width: 600px; margin-left: 4em\"/>\n",
    "\n",
    "<img src=\"resources/Policy_Iteration_2.png\" style=\"width: 600px; margin-left: 4em\"/>\n",
    "\n",
    "## 3.4 Value Iteration\n",
    "\n",
    "In the first graph in secton 3.3, each blow arrow stands for a complete policy evaluation process, where many sweeps are computed until convergence.\n",
    "\n",
    "If we change the trajectory to the following, where each evaluation step brings our estimate a little closer to the value of the current policy but not all the way. Each policy improvement step makes our policy a little more greedy, but not totally greedy. Intuitively, this process should still make progress towards the optimal policy and value function.\n",
    "\n",
    "<img src=\"resources/Value_Iteration_1.png\" style=\"width: 400px; margin-left: 4em\"/>\n",
    "\n",
    "This brings us to our first generalized policy iteration algorithm, called **`value iteration`**.\n",
    "\n",
    "- We still sweep over all the states and greedify with respect to the current value function. \n",
    "- However, we do not run policy evaluation to completion. \n",
    "- We perform just one sweep over all the states. \n",
    "- After that, we greedify again. \n",
    "\n",
    "We can write this as an update rule which applies directly to the state value function. The update does not reference any specific policy, hence the name value iteration.\n",
    "\n",
    "<img src=\"resources/Value_Iteration_2.png\" style=\"width: 600px; margin-left: 4em\"/>\n",
    "\n",
    "Compare to the Policy Iteration, the performence of Value Iteration should be much better.\n",
    "\n",
    "## 3.5 Synchronous DP vs Asynchronous DP\n",
    "\n",
    "Value iteration **`sweeps the entire state space on each iteration`** just like policy iteration. Methods that perform systematic sweeps like this are called **`synchronous`**. \n",
    "\n",
    "- This can be problematic if the statespace is large.\n",
    "- Every sweep could take a very long time.\n",
    "\n",
    "**`Asynchronous dynamic programming`** algorithms update the values of states in any order, they **`do not perform systematic sweeps`**. They might update a given state many times before another is updated even once. In order to guarantee convergence, asynchronous algorithms **`must continue to update the values of all states`**. \n",
    "\n",
    "- can propagate value information quickly through selective updates. \n",
    "- Sometimes this can be **`more efficient than a systematic sweep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Efficiency of DP\n",
    "\n",
    "Let's compare the efficiency of DB against **`Monte Carlo Sampling`** and **`Brute-Force Search`**.\n",
    "\n",
    "### 3.6.1 Monte Carlo, a sample based alternative.\n",
    "\n",
    "The value of each state is treated as a totally independent estimation problem.\n",
    "\n",
    "- recall that the **`value is the expected return`** from a given state.\n",
    "- we gather a large number of returns under pi and **`take their average`**.\n",
    "- this will eventually converge to the state value.\n",
    "- need a **`large number of returns from each state`**.\n",
    "- each return depends on many random actions, selected by pi, as well as many random state transitions due to the dynamics of the MDP. We could be **`dealing with a lot of randomness here`**.\n",
    "- each return might be very different than the true state value. So we may need to **`average many returns before the estimate converges`**, and we have to **`do this for every single state`**.\n",
    "\n",
    "### 3.6.2 Bootstrapping in DP\n",
    "\n",
    "In DP, we **`do not treat`** the evaluation of **`each state as a separate problem`**. We can use the other value estimates we have already worked so hard to compute.\n",
    "\n",
    "This process of **`using the value estimates of successor states to improve our current value estimate`** is known as **`bootstrapping`**. This can be much more efficient than a Monte Carlo method that estimates each value independently.\n",
    "\n",
    "<img src=\"resources/DP_Bootstrapping.png\" style=\"width: 400px; margin-left: 4em\"/>\n",
    "\n",
    "### 3.6.3 The curse of Dimentionality\n",
    "\n",
    "the size of the state space grows exponentially as the number of state variable increases.\n",
    "\n",
    "But this is not an issue with DP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
