{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage\n",
    "\n",
    "In reinforcement learning, the agent generates its own training data by interacting with the world. The agent must learn the consequences of his own actions through trial and error, rather than being told the correct action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. K-armed bandits\n",
    "\n",
    "Let's start with a simple case of **`decision-making under uncertainty`**.\n",
    "\n",
    "In the **`K-armed bandit problem`**:\n",
    "- we have an **`agent`**\n",
    "- who chooses between **`k actions`**\n",
    "- and receives a **`reward`** based on the action it chooses.\n",
    "\n",
    "<img src=\"resources/k_armed_bandit.png\" alt=\"Drawing\" style=\"width: 400px; margin-left: 4em\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 The Action Values\n",
    "\n",
    "### 1.1.1 The real Action Values q*(a)\n",
    "We define the value of selecting an action as the expected reward we receive when taking that action.\n",
    "\n",
    "**`q*(a) = E[ R | A = a ] = Sum[ p(r|a)*r ]`**, for each possible action 1 through k.\n",
    "\n",
    "<img src=\"resources/q*_and_argmax(q*).png\" style=\"width: 400px; margin-left: 4em\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Estimated Action values Q(a)\n",
    "\n",
    "However, q*(a) is unknow in most problems, we need to estimate it.\n",
    "\n",
    "**`Q(a, t) = Sum(R) / (t - 1)`**\n",
    "\n",
    "<img src=\"resources/Q(a)_sample_average.png\" style=\"width: 400px; margin-left: 4em\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 Estimating Action Values Incrementally\n",
    "\n",
    "**`Q(n + 1) = Sum(R) / n = Q(n) + [R(n)- Q(n)] / n`**\n",
    "\n",
    "<img src=\"resources/Q(a)_incremental_update.png\" style=\"width: 400px; margin-left: 4em\"/>\n",
    "\n",
    "<img src=\"resources/Q(a)_incremental_update_rule.png\" style=\"width: 400px; margin-left: 4em\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.4 Constant stepsize could solve the non-stationary problem\n",
    "\n",
    "the influence of initialization of Q goes to zero with more and more data. **`The most recent rewards contribute most`** to our current estimate\n",
    "\n",
    "<img src=\"resources/Q(a)_decaying_past_rewards.png\" style=\"width: 400px; margin-left: 4em\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Action Selection\n",
    "\n",
    "### 1.2.1 The Greedy Action\n",
    "\n",
    "The greedy action is the action that currently has **`the largest estimated value`**.\n",
    "\n",
    "### 1.2.2 Exploitation\n",
    "\n",
    "**`Selecting the greedy action`** means the agent is exploiting its current knowledge. It is trying to **`get the most reward`** it can **`right now`**. We can compute the greedy action **`by taking the argmax`** of our estimated values.\n",
    "\n",
    "### 1.2.3 Exploration\n",
    "\n",
    "Alternatively, the agent may choose to explore by **`choosing a non-greedy action`**. The agent would **`sacrifice immediate reward`**, hoping to gain more information about the other actions for **`potential long term rewards`**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.4 Optimistic Initial Value\n",
    "\n",
    "- encourage early exploration\n",
    "- cannot adapt to non-stationay problem. No exploration at larger steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.5 Epsilon-Greedy\n",
    "\n",
    "- works for non-stationary problem.\n",
    "- however, cannot make full use of exploitation because of epsilon\n",
    "\n",
    "<img src=\"resources/Action_selection_Epsilon_Greedy.png\" style=\"width: 400px; margin-left: 4em\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.6 Upper Confidence Bound\n",
    "\n",
    "- Initially, UCB expore more to systematically reduce uncertainty \n",
    "\n",
    "- UCB exploration reduce over time, thus doesn't fit to non-stationary problem too\n",
    "\n",
    "<img src=\"resources/Action_selection_Upper_Confidence_Bound.png\" style=\"width: 400px; margin-left: 4em\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Markov Decision Process (MDP)\n",
    "\n",
    "The K-armed bandit problem doesn't include many aspects of real-world problems. The agent is presented with the same situation and each time and the same action is always optimal. \n",
    "\n",
    "In many problems, different situations call for different responses. The actions we choose now affect the amount of reward we can get into the future.\n",
    "\n",
    "<img src=\"resources/MDP.png\" style=\"width: 600px; margin-left: 4em\"/>\n",
    "\n",
    "This diagram summarizes the agent environment interaction in the MDP framework. The agent environment interaction generates a trajectory of experience consisting of states, actions, and rewards. Actions influence immediate rewards as well as future states and through those, future reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Dynamics of MDP\n",
    "\n",
    "Given a state S and action a, p tells us the joint probability of next state S prime and reward R.\n",
    "\n",
    "<img src=\"resources/MDP_Dynamics.png\" style=\"width: 600px; margin-left: 4em\"/>\n",
    "\n",
    "The following example shows the Dynamics of the Soda Can Robot problem.\n",
    "\n",
    "States: Battery High, Battery Low\n",
    "\n",
    "Actions: Wait, Search, Recharge\n",
    "\n",
    "\n",
    "<img src=\"resources/MDP_Example.png\" style=\"width: 600px; margin-left: 4em\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Goal: Maximize the expected return\n",
    "\n",
    "Return at time step t, is the **`sum of rewards`** obtained **`after time step t`** \n",
    "\n",
    "**`G(t) = R(t+1) + R(t+2) + R(t+3) + …`**\n",
    "\n",
    "**`E[G(t)] = E[ R(t+1) + R(t+2) + R(t+3) + … ]`**\n",
    "\n",
    "- Episodic task: the agent-environment interaction breaks up into episodes. \n",
    "- Continuing task: the agent-environment interaction goes on indefinitely. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Discounting factor`** gama, makes sure the **`sum of all future goals are finite`**.\n",
    "\n",
    "- **`smaller gama`**, makes the agent **`short sighted`** since it concerns more on immediate rewards.\n",
    "- **`greater gama`**, considers the **`long term rewards`** more.\n",
    "\n",
    "<img src=\"resources/Recursive_nature_of_expected_goal.png\" style=\"width: 400px; margin-left: 4em\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Policy: A distribution over actions for each possible state.\n",
    "\n",
    "- Deterministic policy: maps each state to a single action \n",
    "- Stochastic policy: assigns possibility to each action at each state \n",
    "- A Valid policy depends on only the current state \n",
    "\n",
    "<img src=\"resources/Policy_notation.png\" style=\"width: 600px; margin-left: 4em\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Value functions\n",
    "\n",
    "Value functions are crucial in reinforce learning, they allow an agent to **`query the quality of its current situation`** instead of waiting to observe the long-term outcome. \n",
    "\n",
    "The benefit is twofold:\n",
    "\n",
    "- First, the return is not immediately available\n",
    "- Second, the return may be random due to stochasticity in both the policy and environment dynamics. \n",
    "\n",
    "The value function **`summarizes all the possible futures`** by averaging over returns. \n",
    "\n",
    "Ultimately, we **`care most about learning a good policy`**. Value function enable us to **`judge the quality of different policies`**.\n",
    "\n",
    "### 2.4.1 State Value functions\n",
    "\n",
    "is the expected return from a given state, with respect to the policy Pi.\n",
    "\n",
    "<img src=\"resources/State_value_function.png\" style=\"width: 400px; margin-left: 4em\"/>\n",
    "\n",
    "### 2.4.2 Action Value functions\n",
    "\n",
    "the action value of a state s is the expected return if the agent selects action a and then follows policy Pi.\n",
    "\n",
    "<img src=\"resources/Action_value_function.png\" style=\"width: 400px; margin-left: 4em\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Bellman equations of Value functions\n",
    "\n",
    "They provide **`relationships`** between the **`values of a state or state action pair`** and the **`possible next states or next state action pairs`**\n",
    "\n",
    "### 2.5.1 State value Bellman euqations\n",
    "\n",
    "<img src=\"resources/State_value_Bellman_euqation.png\" style=\"width: 600px; margin-left: 4em\"/>\n",
    "\n",
    "### 2.5.2 Action value Bellman equations\n",
    "\n",
    "<img src=\"resources/Action_value_Bellman_equation.png\" style=\"width: 600px; margin-left: 4em\"/>\n",
    "\n",
    "### 2.5.3 Why Bellman equations?\n",
    "\n",
    "Can use the Bellman equations to **`compute value functions`**.\n",
    "\n",
    "- You can use the Bellman equations to **`solve a value function`** by writing **`a system of linear equations`**.\n",
    "- We can **`only solve small MDPs`** directly with Bellman equations.\n",
    "\n",
    "<img src=\"resources/Bellman_equation_example.png\" style=\"width: 600px; margin-left: 4em\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
