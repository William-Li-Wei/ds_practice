{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage\n",
    "\n",
    "In reinforcement learning, the agent generates its own training data by interacting with the world. The agent must learn the consequences of his own actions through trial and error, rather than being told the correct action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. K-armed bandits\n",
    "\n",
    "Let's start with a simple case of **`decision-making under uncertainty`**.\n",
    "\n",
    "In the **`K-armed bandit problem`**:\n",
    "- we have an **`agent`**\n",
    "- who chooses between **`k actions`**\n",
    "- and receives a **`reward`** based on the action it chooses.\n",
    "\n",
    "<img src=\"resources/k_armed_bandit.png\" alt=\"Drawing\" style=\"width: 400px; margin-left: 4em\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 The Action Values\n",
    "\n",
    "### 0.1.1 The real Action Values q*(a)\n",
    "We define the value of selecting an action as the expected reward we receive when taking that action.\n",
    "\n",
    "**`q*(a) = E[ R | A = a ] = Sum[ p(r|a)*r ]`**, for each possible action 1 through k.\n",
    "\n",
    "<img src=\"resources/q*_and_argmax(q*).png\" style=\"width: 400px; margin-left: 4em\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1.2 Estimated Action values Q(a)\n",
    "\n",
    "However, q*(a) is unknow in most problems, we need to estimate it.\n",
    "\n",
    "**`Q(a, t) = Sum(R) / (t - 1)`**\n",
    "\n",
    "<img src=\"resources/Q(a)_sample_average.png\" style=\"width: 400px; margin-left: 4em\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1.3 Estimating Action Values Incrementally\n",
    "\n",
    "**`Q(n + 1) = Sum(R) / n = Q(n) + [R(n)- Q(n)] / n`**\n",
    "\n",
    "<img src=\"resources/Q(a)_incremental_update.png\" style=\"width: 400px; margin-left: 4em\"/>\n",
    "\n",
    "<img src=\"resources/Q(a)_incremental_update_rule.png\" style=\"width: 400px; margin-left: 4em\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1.4 Constant stepsize could solve the non-stationary problem\n",
    "\n",
    "the influence of initialization of Q goes to zero with more and more data. **The most recent rewards contribute most to our current estimate\n",
    "\n",
    "<img src=\"resources/Q(a)_decaying_past_rewards.png\" style=\"width: 400px; margin-left: 4em\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 Action Selection\n",
    "\n",
    "### 0.2.1 The Greedy Action\n",
    "\n",
    "The greedy action is the action that currently has **`the largest estimated value`**.\n",
    "\n",
    "### 0.2.2 Exploitation\n",
    "\n",
    "**`Selecting the greedy action`** means the agent is exploiting its current knowledge. It is trying to **`get the most reward`** it can **`right now`**. We can compute the greedy action **`by taking the argmax`** of our estimated values.\n",
    "\n",
    "### 0.2.3 Exploration\n",
    "\n",
    "Alternatively, the agent may choose to explore by **`choosing a non-greedy action`**. The agent would **`sacrifice immediate reward`**, hoping to gain more information about the other actions for **`potential long term rewards`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
