{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage\n",
    "\n",
    "In reinforcement learning, the agent generates its own training data by interacting with the world. The agent must learn the consequences of his own actions through trial and error, rather than being told the correct action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. K-armed bandits\n",
    "\n",
    "Let's start with a simple case of **`decision-making under uncertainty`**.\n",
    "\n",
    "In the **`K-armed bandit problem`**:\n",
    "- we have an **`agent`**\n",
    "- who chooses between **`k actions`**\n",
    "- and receives a **`reward`** based on the action it chooses.\n",
    "\n",
    "<img src=\"resources/k_armed_bandit.png\" alt=\"Drawing\" style=\"width: 400px; margin-left: 4em\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 The Action Values\n",
    "\n",
    "### 1.1.1 The real Action Values q*(a)\n",
    "We define the value of selecting an action as the expected reward we receive when taking that action.\n",
    "\n",
    "**`q*(a) = E[ R | A = a ] = Sum[ p(r|a)*r ]`**, for each possible action 1 through k.\n",
    "\n",
    "<img src=\"resources/q*_and_argmax(q*).png\" style=\"width: 400px; margin-left: 4em\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Estimated Action values Q(a)\n",
    "\n",
    "However, q*(a) is unknow in most problems, we need to estimate it.\n",
    "\n",
    "**`Q(a, t) = Sum(R) / (t - 1)`**\n",
    "\n",
    "<img src=\"resources/Q(a)_sample_average.png\" style=\"width: 400px; margin-left: 4em\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 Estimating Action Values Incrementally\n",
    "\n",
    "**`Q(n + 1) = Sum(R) / n = Q(n) + [R(n)- Q(n)] / n`**\n",
    "\n",
    "<img src=\"resources/Q(a)_incremental_update.png\" style=\"width: 400px; margin-left: 4em\"/>\n",
    "\n",
    "<img src=\"resources/Q(a)_incremental_update_rule.png\" style=\"width: 400px; margin-left: 4em\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.4 Constant stepsize could solve the non-stationary problem\n",
    "\n",
    "the influence of initialization of Q goes to zero with more and more data. **`The most recent rewards contribute most`** to our current estimate\n",
    "\n",
    "<img src=\"resources/Q(a)_decaying_past_rewards.png\" style=\"width: 400px; margin-left: 4em\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Action Selection\n",
    "\n",
    "### 1.2.1 The Greedy Action\n",
    "\n",
    "The greedy action is the action that currently has **`the largest estimated value`**.\n",
    "\n",
    "### 1.2.2 Exploitation\n",
    "\n",
    "**`Selecting the greedy action`** means the agent is exploiting its current knowledge. It is trying to **`get the most reward`** it can **`right now`**. We can compute the greedy action **`by taking the argmax`** of our estimated values.\n",
    "\n",
    "### 1.2.3 Exploration\n",
    "\n",
    "Alternatively, the agent may choose to explore by **`choosing a non-greedy action`**. The agent would **`sacrifice immediate reward`**, hoping to gain more information about the other actions for **`potential long term rewards`**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.4 Optimistic Initial Value\n",
    "\n",
    "- encourage early exploration\n",
    "- cannot adapt to non-stationay problem. No exploration at larger steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.5 Epsilon-Greedy\n",
    "\n",
    "- works for non-stationary problem.\n",
    "- however, cannot make full use of exploitation because of epsilon\n",
    "\n",
    "<img src=\"resources/Action_selection_Epsilon_Greedy.png\" style=\"width: 400px; margin-left: 4em\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.6 Upper Confidence Bound\n",
    "\n",
    "- Initially, UCB expore more to systematically reduce uncertainty \n",
    "\n",
    "- UCB exploration reduce over time, thus doesn't fit to non-stationary problem too\n",
    "\n",
    "<img src=\"resources/Action_selection_Upper_Confidence_Bound.png\" style=\"width: 400px; margin-left: 4em\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Markov Decision Process (MDP)\n",
    "\n",
    "The K-armed bandit problem doesn't include many aspects of real-world problems. The agent is presented with the same situation and each time and the same action is always optimal. \n",
    "\n",
    "In many problems, different situations call for different responses. The actions we choose now affect the amount of reward we can get into the future.\n",
    "\n",
    "<img src=\"resources/MDP.png\" style=\"width: 600px; margin-left: 4em\"/>\n",
    "\n",
    "This diagram summarizes the agent environment interaction in the MDP framework. The agent environment interaction generates a trajectory of experience consisting of states, actions, and rewards. Actions influence immediate rewards as well as future states and through those, future reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Dynamics of MDP\n",
    "\n",
    "Given a state S and action a, p tells us the joint probability of next state S prime and reward R.\n",
    "\n",
    "<img src=\"resources/MDP_Dynamics.png\" style=\"width: 600px; margin-left: 4em\"/>\n",
    "\n",
    "The following example shows the Dynamics of the Soda Can Robot problem.\n",
    "\n",
    "States: Battery High, Battery Low\n",
    "\n",
    "Actions: Wait, Search, Recharge\n",
    "\n",
    "\n",
    "<img src=\"resources/MDP_Example.png\" style=\"width: 600px; margin-left: 4em\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Goal: Maximize the expected return\n",
    "\n",
    "Return at time step t, is the **`sum of rewards`** obtained **`after time step t`** \n",
    "\n",
    "**`G(t) = R(t+1) + R(t+2) + R(t+3) + …`**\n",
    "\n",
    "**`E[G(t)] = E[ R(t+1) + R(t+2) + R(t+3) + … ]`**\n",
    "\n",
    "- Episodic task: the agent-environment interaction breaks up into episodes. \n",
    "- Continuing task: the agent-environment interaction goes on indefinitely. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Recursive nature of Expected Goal with Discounting\n",
    "\n",
    "Discounting factor gama, makes sure the sum of all future goals are finite.\n",
    "\n",
    "<img src=\"resources/Recursive_nature_of_expected_goal.png\" style=\"width: 400px; margin-left: 4em\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
