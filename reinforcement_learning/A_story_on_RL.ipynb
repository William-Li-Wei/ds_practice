{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage\n",
    "\n",
    "In reinforcement learning, the agent generates its own training data by interacting with the world. The agent must learn the consequences of his own actions through trial and error, rather than being told the correct action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. K-armed bandits\n",
    "\n",
    "Let's start with a simple case of **`decision-making under uncertainty`**.\n",
    "\n",
    "In the **`K-armed bandit problem`**:\n",
    "- we have an **`agent`**\n",
    "- who chooses between **`k actions`**\n",
    "- and receives a **`reward`** based on the action it chooses.\n",
    "\n",
    "<img src=\"resources/k_armed_bandit.png\" alt=\"Drawing\" style=\"width: 400px; margin-left: 4em\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 The Action Values\n",
    "\n",
    "### 1.1.1 The real Action Values q*(a)\n",
    "We define the value of selecting an action as the expected reward we receive when taking that action.\n",
    "\n",
    "**`q*(a) = E[ R | A = a ] = Sum[ p(r|a)*r ]`**, for each possible action 1 through k.\n",
    "\n",
    "<img src=\"resources/q*_and_argmax(q*).png\" style=\"width: 400px; margin-left: 4em\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Estimated Action values Q(a)\n",
    "\n",
    "However, q*(a) is unknow in most problems, we need to estimate it.\n",
    "\n",
    "**`Q(a, t) = Sum(R) / (t - 1)`**\n",
    "\n",
    "<img src=\"resources/Q(a)_sample_average.png\" style=\"width: 400px; margin-left: 4em\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 Estimating Action Values Incrementally\n",
    "\n",
    "**`Q(n + 1) = Sum(R) / n = Q(n) + [R(n)- Q(n)] / n`**\n",
    "\n",
    "<img src=\"resources/Q(a)_incremental_update.png\" style=\"width: 400px; margin-left: 4em\"/>\n",
    "\n",
    "<img src=\"resources/Q(a)_incremental_update_rule.png\" style=\"width: 400px; margin-left: 4em\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.4 Constant stepsize could solve the non-stationary problem\n",
    "\n",
    "the influence of initialization of Q goes to zero with more and more data. **`The most recent rewards contribute most`** to our current estimate\n",
    "\n",
    "<img src=\"resources/Q(a)_decaying_past_rewards.png\" style=\"width: 400px; margin-left: 4em\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Action Selection\n",
    "\n",
    "### 1.2.1 The Greedy Action\n",
    "\n",
    "The greedy action is the action that currently has **`the largest estimated value`**.\n",
    "\n",
    "### 1.2.2 Exploitation\n",
    "\n",
    "**`Selecting the greedy action`** means the agent is exploiting its current knowledge. It is trying to **`get the most reward`** it can **`right now`**. We can compute the greedy action **`by taking the argmax`** of our estimated values.\n",
    "\n",
    "### 1.2.3 Exploration\n",
    "\n",
    "Alternatively, the agent may choose to explore by **`choosing a non-greedy action`**. The agent would **`sacrifice immediate reward`**, hoping to gain more information about the other actions for **`potential long term rewards`**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.4 Optimistic Initial Value\n",
    "\n",
    "- encourage early exploration\n",
    "- cannot adapt to non-stationay problem. No exploration at larger steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.5 Epsilon-Greedy\n",
    "\n",
    "- works for non-stationary problem.\n",
    "- however, cannot make full use of exploitation because of epsilon\n",
    "\n",
    "<img src=\"resources/Action_selection_Epsilon_Greedy.png\" style=\"width: 400px; margin-left: 4em\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.6 Upper Confidence Bound\n",
    "\n",
    "- Initially, UCB expore more to systematically reduce uncertainty \n",
    "\n",
    "- UCB exploration reduce over time, thus doesn't fit to non-stationary problem too\n",
    "\n",
    "<img src=\"resources/Action_selection_Upper_Confidence_Bound.png\" style=\"width: 400px; margin-left: 4em\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Markov Decision Process (MDP)\n",
    "\n",
    "The K-armed bandit problem doesn't include many aspects of real-world problems. The agent is presented with the same situation and each time and the same action is always optimal. \n",
    "\n",
    "In many problems, different situations call for different responses. The actions we choose now affect the amount of reward we can get into the future.\n",
    "\n",
    "<img src=\"resources/MDP.png\" style=\"width: 600px; margin-left: 4em\"/>\n",
    "\n",
    "This diagram summarizes the agent environment interaction in the MDP framework. The agent environment interaction generates a trajectory of experience consisting of states, actions, and rewards. Actions influence immediate rewards as well as future states and through those, future reward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Dynamics of MDP\n",
    "\n",
    "Given a state S and action a, p tells us the joint probability of next state S prime and reward R.\n",
    "\n",
    "<img src=\"resources/MDP_Dynamics.png\" style=\"width: 600px; margin-left: 4em\"/>\n",
    "\n",
    "The following example shows the Dynamics of the Soda Can Robot problem.\n",
    "\n",
    "States: Battery High, Battery Low\n",
    "\n",
    "Actions: Wait, Search, Recharge\n",
    "\n",
    "\n",
    "<img src=\"resources/MDP_Example.png\" style=\"width: 600px; margin-left: 4em\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Goal: Maximize the expected return\n",
    "\n",
    "Return at time step t, is the **`sum of rewards`** obtained **`after time step t`** \n",
    "\n",
    "**`G(t) = R(t+1) + R(t+2) + R(t+3) + …`**\n",
    "\n",
    "**`E[G(t)] = E[ R(t+1) + R(t+2) + R(t+3) + … ]`**\n",
    "\n",
    "- Episodic task: the agent-environment interaction breaks up into episodes. \n",
    "- Continuing task: the agent-environment interaction goes on indefinitely. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`Discounting factor`** gama, makes sure the **`sum of all future goals are finite`**.\n",
    "\n",
    "- **`smaller gama`**, makes the agent **`short sighted`** since it concerns more on immediate rewards.\n",
    "- **`greater gama`**, considers the **`long term rewards`** more.\n",
    "\n",
    "<img src=\"resources/Recursive_nature_of_expected_goal.png\" style=\"width: 400px; margin-left: 4em\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Policy: A distribution over actions for each possible state.\n",
    "\n",
    "- Deterministic policy: maps each state to a single action \n",
    "- Stochastic policy: assigns possibility to each action at each state \n",
    "- A Valid policy depends on only the current state \n",
    "\n",
    "<img src=\"resources/Policy_notation.png\" style=\"width: 600px; margin-left: 4em\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Value functions\n",
    "\n",
    "Value functions are crucial in reinforce learning, they allow an agent to **`query the quality of its current situation`** instead of waiting to observe the long-term outcome. \n",
    "\n",
    "The benefit is twofold:\n",
    "\n",
    "- First, the return is not immediately available\n",
    "- Second, the return may be random due to stochasticity in both the policy and environment dynamics. \n",
    "\n",
    "The value function **`summarizes all the possible futures`** by averaging over returns. \n",
    "\n",
    "Ultimately, we **`care most about learning a good policy`**. Value function enable us to **`judge the quality of different policies`**.\n",
    "\n",
    "### 2.4.1 State Value functions\n",
    "\n",
    "is the expected return from a given state, with respect to the policy Pi.\n",
    "\n",
    "<img src=\"resources/State_value_function.png\" style=\"width: 400px; margin-left: 4em\"/>\n",
    "\n",
    "### 2.4.2 Action Value functions\n",
    "\n",
    "the action value of a state s is the expected return if the agent selects action a and then follows policy Pi.\n",
    "\n",
    "<img src=\"resources/Action_value_function.png\" style=\"width: 400px; margin-left: 4em\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Bellman equations of Value functions\n",
    "\n",
    "They provide **`relationships`** between the **`values of a state or state action pair`** and the **`possible next states or next state action pairs`**\n",
    "\n",
    "### 2.5.1 State value Bellman euqations\n",
    "\n",
    "<img src=\"resources/State_value_Bellman_euqation.png\" style=\"width: 600px; margin-left: 4em\"/>\n",
    "\n",
    "### 2.5.2 Action value Bellman equations\n",
    "\n",
    "<img src=\"resources/Action_value_Bellman_equation.png\" style=\"width: 600px; margin-left: 4em\"/>\n",
    "\n",
    "### 2.5.3 Why Bellman equations?\n",
    "\n",
    "Can use the Bellman equations to **`compute value functions`**.\n",
    "\n",
    "- You can use the Bellman equations to **`solve a value function`** by writing **`a system of linear equations`**.\n",
    "- We can **`only solve small MDPs`** directly with Bellman equations.\n",
    "\n",
    "<img src=\"resources/Bellman_equation_example.png\" style=\"width: 600px; margin-left: 4em\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Optimal Policy and Optimal value functions\n",
    "\n",
    "- An **`optimal policy`** is defined as the policy with the **`highest possible value`** function in **`all states`**. \n",
    "- **`At lease one optimal policy`** always exists, but there **`may be more`** than one. \n",
    "- The exponential number of possible policies makes searching for the optimal policy by brute-force intractable. \n",
    "\n",
    "A **`deterministic optimal policy`** will assign **`Probability 1`**, for an action that achieves the **`highest value`** and Probability **`0, for all other actions`**.\n",
    "\n",
    "Finding an Optimal Policy is the goal of our Reinforcement Learning problems.\n",
    "\n",
    "### 2.6.1 Bellman optimality equation for v*\n",
    "\n",
    "<img src=\"resources/Optimal_State_value_function_Bellman.png\" style=\"width: 400px; margin-left: 4em\"/>\n",
    "\n",
    "### 2.6.2 Bellman optimality equation for q*\n",
    "\n",
    "<img src=\"resources/Optimal_Action_value_function_Bellman.png\" style=\"width: 400px; margin-left: 4em\"/>\n",
    "\n",
    "The Bellman optimality equations **`relate the value of a state`** or **`state-action pair`**, **`to it's possible successors`** under any optimal policy.\n",
    "\n",
    "### 2.6.3 Determin optimal policy\n",
    "\n",
    "In general, **`having v* or q*`** makes it relatively easy to **`work out the optimal policy`** as long as we also have **`access to the dynamics function p`**.\n",
    "\n",
    "<img src=\"resources/Determin_optimal_policy_from_optimal_value_functions.png\" style=\"width: 600px; margin-left: 4em\"/>\n",
    "\n",
    "However, v* and q* are unknown, we need to start from the initial policy and then iterate between Policy Evaluation and Policy Improvement to get the optimoal policy. See more detils in Dynamic Programming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Dynamic Programming\n",
    "\n",
    "- **`Policy evaluation`** is the task of **`determining the state-vale`** function V(Pi), for a particular policy Pi. \n",
    "- **`Control`** is the task of **`improving an existing policy`**. \n",
    "\n",
    "Dynamic programming techniques can be used to solve both these tasks, if we **`have access to the dynamics function P`**. \n",
    "\n",
    "## 3.1 Iterative Policy Evaluation\n",
    "\n",
    "We take the Bellman equation and directly use it as an update rule. This will produce a sequence of better and better approximations to the value function.\n",
    "\n",
    "<img src=\"resources/Policy_Evaluation_1.png\" style=\"width: 400px; margin-left: 4em\"/>\n",
    "\n",
    "It can be proven that for any choice of v(0), v(k) will converge to v(pi) in the limit as k approaches infinity.\n",
    "\n",
    "<img src=\"resources/Policy_Evaluation_2.png\" style=\"width: 400px; margin-left: 4em\"/>\n",
    "\n",
    "The complete iteration of Policy Evaluation is as followed:\n",
    "\n",
    "<img src=\"resources/Policy_Evaluation_3.png\" style=\"width: 600px; margin-left: 4em\"/>\n",
    "\n",
    "### 3.1.2 Policy Evaluation Example\n",
    "\n",
    "In the grid world example, the top-left and the bottom-right blocks are the termination states. The actions are going up, right, down or left from each state, hitting the boundry makes the state unchanged. The policy Pi is the uniform random policy. The initial value of each state under Pi is 0.\n",
    "\n",
    "Calculating the first state gives -1.\n",
    "\n",
    "<img src=\"resources/Policy_Evaluation_Example_1.png\" style=\"width: 600px; margin-left: 4em\"/>\n",
    "\n",
    "After calculating the values of all states (all -1 in this example), we finished one sweep. The max difference delta here is 1.\n",
    "\n",
    "<img src=\"resources/Policy_Evaluation_Example_2.png\" style=\"width: 600px; margin-left: 4em\"/>\n",
    "\n",
    "Repeating the above iteration for another sweep we got:\n",
    "\n",
    "<img src=\"resources/Policy_Evaluation_Example_3.png\" style=\"width: 600px; margin-left: 4em\"/>\n",
    "\n",
    "If we set the minimal difference theta to 0.001 and continue the iteration, eventually we will get the estimated policy evaluation:\n",
    "\n",
    "<img src=\"resources/Policy_Evaluation_Example_4.png\" style=\"width: 600px; margin-left: 4em\"/>\n",
    "\n",
    "\n",
    "\n",
    "## 3.2 Policy Improvement\n",
    "\n",
    "Recall that given v*, we can find the optimal policy by choosing the Greedy action, which maximizes the Bellman's optimality equation in each state.\n",
    "\n",
    "Selecting the **`greedy action`** with respect to the value function v(pi), must **`result in a different policy`** other than pi. **`If this greedification doesn't change pi`**, **`then pi was already greedy`** with respect to its own value function, in other words, pi obeys the Bellman's optimal equition and **`therefore is already optimal`**.\n",
    "\n",
    "<img src=\"resources/Policy_Improvement_1.png\" style=\"width: 600px; margin-left: 4em\"/>\n",
    "\n",
    "### 3.2.1 Policy Improvement Theorem\n",
    "\n",
    "The new policy obtained in the above way must be a strict improvement on Pi, unless Pi was already optimal. This is a consequence of the policy improvement theorem.\n",
    "\n",
    "<img src=\"resources/Policy_Improvement_Theorem.png\" style=\"width: 400px; margin-left: 4em\"/>\n",
    "\n",
    "### 3.2.2 Policy Improvement Example\n",
    "\n",
    "Pi is the uniform random policy, and the numbers are the evaluation of each state. The arrows show the argmax actions and form a new polic Pi*.\n",
    "\n",
    "<img src=\"resources/Policy_Improvement_Example.png\" style=\"width: 600px; margin-left: 4em\"/>\n",
    "\n",
    "## 3.3 Policy Iteration\n",
    "\n",
    "- start from the initial policy Pi1\n",
    "- evaluate the value function V(Pi1)\n",
    "- then take argmax actions to get a better policy Pi2\n",
    "- then evaluate Pi2 and take argmax actions to get Pi3\n",
    "- repeat the process until we get the optimal policy Pi*\n",
    "\n",
    "<img src=\"resources/Policy_Iteration_1.png\" style=\"width: 600px; margin-left: 4em\"/>\n",
    "\n",
    "<img src=\"resources/Policy_Iteration_2.png\" style=\"width: 600px; margin-left: 4em\"/>\n",
    "\n",
    "## 3.4 Value Iteration\n",
    "\n",
    "In the first graph in secton 3.3, each blow arrow stands for a complete policy evaluation process, where many sweeps are computed until convergence.\n",
    "\n",
    "If we change the trajectory to the following, where each evaluation step brings our estimate a little closer to the value of the current policy but not all the way. Each policy improvement step makes our policy a little more greedy, but not totally greedy. Intuitively, this process should still make progress towards the optimal policy and value function.\n",
    "\n",
    "<img src=\"resources/Value_Iteration_1.png\" style=\"width: 400px; margin-left: 4em\"/>\n",
    "\n",
    "This brings us to our first generalized policy iteration algorithm, called **`value iteration`**.\n",
    "\n",
    "- We still sweep over all the states and greedify with respect to the current value function. \n",
    "- However, we do not run policy evaluation to completion. \n",
    "- We perform just one sweep over all the states. \n",
    "- After that, we greedify again. \n",
    "\n",
    "We can write this as an update rule which applies directly to the state value function. The update does not reference any specific policy, hence the name value iteration.\n",
    "\n",
    "<img src=\"resources/Value_Iteration_2.png\" style=\"width: 600px; margin-left: 4em\"/>\n",
    "\n",
    "Compare to the Policy Iteration, the performence of Value Iteration should be much better.\n",
    "\n",
    "## 3.5 Synchronous DP vs Asynchronous DP\n",
    "\n",
    "Value iteration **`sweeps the entire state space on each iteration`** just like policy iteration. Methods that perform systematic sweeps like this are called **`synchronous`**. \n",
    "\n",
    "- This can be problematic if the statespace is large.\n",
    "- Every sweep could take a very long time.\n",
    "\n",
    "**`Asynchronous dynamic programming`** algorithms update the values of states in any order, they **`do not perform systematic sweeps`**. They might update a given state many times before another is updated even once. In order to guarantee convergence, asynchronous algorithms **`must continue to update the values of all states`**. \n",
    "\n",
    "- can propagate value information quickly through selective updates. \n",
    "- Sometimes this can be **`more efficient than a systematic sweep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Efficiency of DP\n",
    "\n",
    "Let's compare the efficiency of DB against **`Monte Carlo Sampling`** and **`Brute-Force Search`**.\n",
    "\n",
    "### 3.6.1 Monte Carlo, a sample based alternative.\n",
    "\n",
    "The value of each state is treated as a totally independent estimation problem.\n",
    "\n",
    "- recall that the **`value is the expected return`** from a given state.\n",
    "- we gather a large number of returns under pi and **`take their average`**.\n",
    "- this will eventually converge to the state value.\n",
    "- need a **`large number of returns from each state`**.\n",
    "- each return depends on many random actions, selected by pi, as well as many random state transitions due to the dynamics of the MDP. We could be **`dealing with a lot of randomness here`**.\n",
    "- each return might be very different than the true state value. So we may need to **`average many returns before the estimate converges`**, and we have to **`do this for every single state`**.\n",
    "\n",
    "### 3.6.2 Bootstrapping in DP\n",
    "\n",
    "In DP, we **`do not treat`** the evaluation of **`each state as a separate problem`**. We can use the other value estimates we have already worked so hard to compute.\n",
    "\n",
    "This process of **`using the value estimates of successor states to improve our current value estimate`** is known as **`bootstrapping`**. This can be much more efficient than a Monte Carlo method that estimates each value independently.\n",
    "\n",
    "<img src=\"resources/DP_Bootstrapping.png\" style=\"width: 400px; margin-left: 4em\"/>\n",
    "\n",
    "### 3.6.3 The curse of Dimentionality\n",
    "\n",
    "the size of the state space grows exponentially as the number of state variable increases.\n",
    "\n",
    "But this is not an issue with DP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Sample based Learning methods\n",
    "\n",
    "**`DP`** powerful and efficient, however it **`is depending on the environment dynamics`**(the environment's transision probabilities), which is mostly not available.\n",
    "\n",
    "In these cases, Sample based learning methods can estimate values directly from experience, from sequences of states, actions and rewards.\n",
    "\n",
    "## 4.1 Monte Carlo for prediction\n",
    "\n",
    "Without knowing the probability of transisions, Monte Carlo methods **`estimate values by averaging`** over a large number of **`random samples`**. \n",
    "\n",
    "- It first observes multiple returns from the same state. \n",
    "- Then averages the returns to estimate the expected return from that state.\n",
    "\n",
    "As the number of samples increases, the average tends to get closer and closer to the expected return. The more returns the agent observes from a state, the more likely it is that the sample average is close to the state value. \n",
    "\n",
    "- These **`returns`** can only be **`observed at the end of an episode`**. \n",
    "- So we will **`focus on`** Monte Carlo methods for **`episodic tasks`**.\n",
    "\n",
    "### 4.1.1 Monte Carlo prediction algothrim\n",
    "\n",
    "<img src=\"resources/Monte_Carlo_Method.png\" style=\"width: 600px; margin-left: 4em\"/>\n",
    "\n",
    "- By working **`backwards from the terminal step`**, we can efficiently compute the returns for each state encountered during the episode.\n",
    "- After **`updating the values for all the states`** visited in the current episode, we **`repeat`** the whole process **`over many episodes`** and eventually learn a good estimate for the value function.\n",
    "\n",
    "Can we **`avoid keeping all the sampled returns`** in a list? In fact, we can. We can **`incrementally update`** the sample average estimated using the above formula.\n",
    "\n",
    "### 4.1.2 Monte Carlo prediction example: Black Jack\n",
    "\n",
    "- **`Undiscounted MDP`** where each game of blackjack corresponds to an episode. \n",
    "- **`Rewards`** are given **`at the end of the game`** with **`-1 for loss, 0 for draw, 1 for win`**. \n",
    "- The player has **`two actions, hit or stick`**. \n",
    "- The player decides to hit or stick **`based on three variables`**: whether they have a **`usable ace`** which is an ace that can count as 11 without their sum exceeding 21, the **`sum of their cards`** and **`the card the dealer shows`**. \n",
    "- There are 200 states in total. \n",
    "- We assume the cards are dealt from the deck with replacement. This means that there's no point in keeping track of the cards that had been dealt and that the state respects the Markov property. \n",
    "\n",
    "Let's use Monte Carlo to **`learn the value function for the policy that sticks when the player sum is 20 or 21`**.\n",
    "\n",
    "<img src=\"resources/Monte_Carlo_prediction_Example_black_jack_1.png\" style=\"width: 600px; margin-left: 4em\"/>\n",
    "\n",
    "- State A is the last none Terminal state, T-1.\n",
    "- State B is the second last none Terminal state, T-2.\n",
    "\n",
    "After 500000 episodes, we can see that **`under given policy`**:\n",
    "- values are high for the states with sum of 20 or 21.\n",
    "- values are low for other states\n",
    "<img src=\"resources/Monte_Carlo_prediction_Example_black_jack_2.png\" style=\"width: 600px; margin-left: 4em\"/>\n",
    "\n",
    "## 4.2 Maintaing exploration\n",
    "\n",
    "- If an **`action is never selected`** by the policy, the agent will never observe returns corresponding to that action, thus **`no estimation`**.\n",
    "- Must **`try all the actions in each state`** in order **`to learn their values`**.\n",
    "\n",
    "### 4.2.1 Exploring Starts\n",
    "\n",
    "is one way to maintain exploration.We must guarantee that **`episodes start in every state-action pair`**. Afterwards, the agent simply follows its policy.\n",
    "\n",
    "- Exploring starts will **`randomly sample a state and action at the start of an episode`**.\n",
    "- This may **`not always be possible`**.\n",
    "\n",
    "<img src=\"resources/Exploring_Starts.png\" style=\"width: 400px; margin-left: 4em\"/>\n",
    "\n",
    "### 4.2.2 Epsilon Soft ( Greedy)\n",
    "\n",
    "is another exploration strategy to evaluate stochastic policies. \n",
    "\n",
    "- Epsilon soft policies force the agent to **`continually explore`**, that means we can drop the exploring starts requirement from the Monte Carlo control algorithm. \n",
    "- An Epsilon soft policy **`assigns nonzero probability to each action in every state`** and continue to **`visit all state action pairs indefinitely`**.\n",
    "- Epsilon soft policies are **`always stochastic`**. They will eventually try all the actions.\n",
    "\n",
    "<img src=\"resources/Epsilon_Greedy_vs_Deterministic.png\" style=\"width: 600px; margin-left: 4em\"/>\n",
    "\n",
    "<img src=\"resources/Epsilon_Greedy_vs_Deterministic_2.png\" style=\"width: 600px; margin-left: 4em\"/>\n",
    "\n",
    "- With Epsilon Soft method, it's **`impossible to converge to a deterministic optimal policy`**.\n",
    "- Exploring starts can be used to find the optimal policy, but Epsilon soft policies can **`only find the optimal Epsilon soft policy`**.\n",
    "- The optimal Epsilon soft policy performs worse than the optimal policy in general. However, it often performs reasonably well and allows us to get rid of exploring starts.\n",
    "\n",
    "### 4.2.3 Compare Exploring Starts and Epsilon Soft\n",
    "\n",
    "#### Exploring Starts\n",
    "- can learn the optimal deterministic policy\n",
    "- not always possible\n",
    "\n",
    "#### Epsilon Soft\n",
    "- can learn the **`near optimal`** stochastic policy\n",
    "- performs reasonably well\n",
    "\n",
    "\n",
    "## 4.3 Monte Carlo for control\n",
    "\n",
    "To Estimate action values, we collect **`returns`** following a policy from **`state-action pair`** and then **`take their average`**.\n",
    "\n",
    "Action values are **`useful for learning a policy`**. They allow us to **`compare different actions in the same state`**.\n",
    "\n",
    "### 4.3.1 Generalized Policy Iteration (GPI) with Monte Carlo\n",
    "\n",
    "GPI includes a **`policy evaluation`** and a **`policy improvement`** step. It produces sequences of policies that are at least as good as the policies before them.\n",
    "\n",
    "1. Policy evaluation, use Monte Carlo to estimate the action values. \n",
    "2. The value estimates need only improve a little, not all the way to the correct action values.\n",
    "3. All that is required for convergence is that the estimates continue to improve. \n",
    "4. Monte Carlo control methods combined policy improvement and policy evaluation on an episode-by-episode basis.\n",
    "\n",
    "<img src=\"resources/GPI_Monte_Carlo.png\" style=\"width: 600px; margin-left: 4em\"/>\n",
    "\n",
    "### 4.3.2 Monte Carlo with Exploring Starts\n",
    "<img src=\"resources/Monte_Carlo_ES.png\" style=\"width: 600px; margin-left: 4em\"/>\n",
    "\n",
    "### 4.3.3 Monte Carlo with Epsilon Soft\n",
    "\n",
    "<img src=\"resources/Monte_Carlo_Epsilon_Soft.png\" style=\"width: 600px; margin-left: 4em\"/>\n",
    "\n",
    "It learns near optimal policies.\n",
    "\n",
    "### 4.3.4 Monte Carlo Control Example: Black Jack\n",
    "\n",
    "<img src=\"resources/Monte_Carlo_control_Example_black_jack_1.png\" style=\"width: 600px; margin-left: 4em\"/>\n",
    "\n",
    "<img src=\"resources/Monte_Carlo_control_Example_black_jack_2.png\" style=\"width: 600px; margin-left: 4em\"/>\n",
    "\n",
    "With a usable ace:\n",
    "- for most dealer cards, the agent hits until it has the sum near 19\n",
    "- the agent has a lot more flexibility in calculating the sum of its cards\n",
    "- so the policy is much more aggressive\n",
    "\n",
    "Without a usable ace:\n",
    "- the policy depends a lot more on the cards the dealer is showing. \n",
    "- the agent sticks when its sum is 13 or greater and the dealer has a low card, like a two or three. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Off-policy learning\n",
    "\n",
    "So far we've been only talking about On-policy learning, e.g. Epsilon soft policy.\n",
    "\n",
    "However, epsilon soft policy has disadvantages:\n",
    "- it's **`not optimal for exploring`** to find the best actions\n",
    "- it's **`not optimal for obtaining rewards`**\n",
    "\n",
    "Off-policy learning can deal with the exploration problem.\n",
    "\n",
    "#### On-policy learning: \n",
    "the agent learns about the policy used to generate the data\n",
    "\n",
    "#### Off-policy learning:\n",
    "the agent learns about a policy from data generated by following a different policy. That is the policy that we are learning is off the policy that we are using for Action selection. For example, you could learn the optimal policy while following a totally random policy.\n",
    "\n",
    "- **`Target policy`** Pi(a|s): the policy that the agent is learning. We'll learn values for this policy, e.g. **`the optimal policy`**.\n",
    "- **`Behavior policy`** B(a|s): the policy that the agent selects actions from, generally **`an explory policy`**.\n",
    "\n",
    "The behavior policy must cover the target policy.\n",
    "\n",
    "### 4.4.1 Importance Sampling\n",
    "\n",
    "allows us to do off-policy learning, learning with one policy while following another.\n",
    "\n",
    "#### state the problem\n",
    "Given some random **`variable x`** that being **`sampled from`** a probability **`distribution b`**. We want to **`estimate the expected value of x`** but **`with respect to the target distribution Pi`**. \n",
    "\n",
    "Because x is drawn from b, we cannot simply use the sample average to compute the expectation under Pi. This sample average will give us the expected value under b instead.\n",
    "\n",
    "<img src=\"resources/Importance_Sampling_1.png\" style=\"width: 400px; margin-left: 4em\"/>\n",
    "\n",
    "Denote **`pi(x) / b(x)`** as **`rho(x)`**, stands for the importance sampling ratio.\n",
    "\n",
    "If we treat x times the importance sampling ratio as a new random variable, times the probability of observing x, we can then rewrite this sum as an expectation under b.\n",
    "\n",
    "<img src=\"resources/Importance_Sampling_2.png\" style=\"width: 400px; margin-left: 4em\"/>\n",
    "\n",
    "#### Importance Sampling Example\n",
    "\n",
    "<img src=\"resources/Importance_Sampling_Example.png\" style=\"width: 600px; margin-left: 4em\"/>\n",
    "\n",
    "### 4.4.2 Off-policy Monte Carlo prediction\n",
    "\n",
    "<img src=\"resources/Off_Policy_Monte_Carlo_Prediction_1.png\" style=\"width: 400px; margin-left: 4em\"/>\n",
    "\n",
    "<img src=\"resources/Off_Policy_Monte_Carlo_Prediction_2.png\" style=\"width: 600px; margin-left: 4em\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Temporal Difference Learning (TD)\n",
    "\n",
    "In **`Monte Carlo`**, we use this formula to **`incrementally update our estimated value`**.\n",
    "\n",
    "<img src=\"resources/Monte_Carlo_Incremental_Value_Estimation.png\" style=\"width: 400px; margin-left: 4em\"/>\n",
    "\n",
    "- a **`constant step size`**, means getting **`estimate without saving lists of returns`**.\n",
    "- to **`compute the return`**, we **`have to take`** samples of **`full trajectories`**. This means we don't learn during the episode\n",
    "\n",
    "But we want to be able to learn incrementally before the end of the episode. We must come up with a new update target to achieve this goal.\n",
    "\n",
    "### 4.5.1 TD prediction\n",
    "Let's try to make the value function recursive.\n",
    "\n",
    "<img src=\"resources/TD_Incremental_Value_Estimation.png\" style=\"width: 400px; margin-left: 4em\"/>\n",
    "\n",
    "Replacing the return at time t with the reward plus the estimate of the return in the next state, we got the TD update rule. Delta is the TD error.\n",
    "<img src=\"resources/TD_Update.png\" style=\"width: 600px; margin-left: 4em\"/>\n",
    "\n",
    "<img src=\"resources/TD_Value_Estimation.png\" style=\"width: 600px; margin-left: 4em\"/>\n",
    "\n",
    "### 4.5.2 Advantages of TD over DP and MC\n",
    "\n",
    "- unlike DP, TD **`does not require a model of the environment`**.\n",
    "- unlike MC, TD can **`learn online and incrementally`** without waiting to the end of the episode.\n",
    "- TD **`converges faster`** than MC.\n",
    "\n",
    "<img src=\"resources/TD_vs_MC_performance.png\" style=\"width: 600px; margin-left: 4em\"/>\n",
    "\n",
    "- **`larger learning rate, faster but higher error`**.\n",
    "- **`smaller learning rate, slower but lower error`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
